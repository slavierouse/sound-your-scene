{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Explorer and Debugger\n",
    "\n",
    "This notebook provides comprehensive testing and debugging capabilities for the SoundByMood API backend.\n",
    "\n",
    "## Sections:\n",
    "1. **Direct Search Service Testing** - Manual testing of search functionality\n",
    "2. **Service Component Debugging** - Test music_service and llm_service individually\n",
    "3. **Storage Persistence Debugging** - Inspect in-memory storage state\n",
    "4. **End-to-End API Testing** - Full endpoint simulation with storage inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Direct Search Service Testing\n",
    "\n",
    "Manually test the search functionality without going through the API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Add project root to path and import services\nimport sys\nimport os\nsys.path.append('..')\n\n# Environment setup\nfrom dotenv import load_dotenv\nload_dotenv('../.env', override=True)\n\n# Import services with correct module paths\nfrom api.music_service import MusicService\nfrom api.llm_service import LLMService\nfrom api.search_service import initialize_services\n\n# Initialize services\nmusic_service = MusicService()\nllm_service = LLMService()\n\nprint(\"Initializing services...\")\nmusic_service.initialize('../data/main_df.csv')\nllm_service.initialize()\nprint(\"✅ Services initialized successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Testing query: 'brooding electronic music for coding'\n",
      "============================================================\n",
      "📝 Initial prompt: User query: brooding electronic music for coding\n",
      "Return ONLY JSON per schema.\n",
      "🤖 LLM Response:\n",
      "   User Message: I've set filters to find music that is less positive and energetic, and more instrumental with minimal vocals, fitting the 'brooding' and 'for coding' aspects. It's specifically tailored to electronic genres like EDM, ambient, house, and techno.\n",
      "   Reflection: The most important decisions were interpreting 'brooding' as low valence and low energy, and 'for coding' as highly instrumental with low speechiness to minimize distraction. 'Electronic music' was directly mapped to genre filters. Key filters set were: valence_max_decile to 4 and valence_decile_weight to -50; energy_max_decile to 5 and energy_decile_weight to -40; instrumentalness_min_decile to 7 and instrumentalness_decile_weight to 50; and speechiness_max_decile to 3 and speechiness_decile_weight to -50. Additionally, genre filters for electronic music were critical. To make this better, one could consider filtering by track popularity (e.g., views_min_decile) if the user implies wanting well-known tracks, or perhaps a slight negative weight for liveness to avoid live recordings if 'for coding' implies studio-quality focus.\n",
      "📊 Search Results:\n",
      "   Total Results: 0\n",
      "   Summary: {'result_count': 0}\n",
      "❌ No results found\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Manual query with auto-refinement simulation\n",
    "test_query = \"brooding electronic music for coding\"\n",
    "\n",
    "print(f\"🔍 Testing query: '{test_query}'\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Get initial LLM response\n",
    "initial_prompt = llm_service.create_initial_prompt(test_query)\n",
    "print(f\"📝 Initial prompt: {initial_prompt}\")\n",
    "\n",
    "filters_json = await llm_service.query_llm(initial_prompt)\n",
    "print(f\"🤖 LLM Response:\")\n",
    "print(f\"   User Message: {filters_json.get('user_message')}\")\n",
    "print(f\"   Reflection: {filters_json.get('reflection')}\")\n",
    "\n",
    "# Step 2: Apply filters and get results\n",
    "search_result = music_service.search(filters_json)\n",
    "results_df = search_result[\"results\"]\n",
    "summary = search_result[\"summary\"]\n",
    "\n",
    "print(f\"📊 Search Results:\")\n",
    "print(f\"   Total Results: {len(results_df)}\")\n",
    "print(f\"   Summary: {summary}\")\n",
    "\n",
    "# Step 3: Show top results\n",
    "if len(results_df) > 0:\n",
    "    top_5 = results_df.sort_values('relevance_score', ascending=False).head(5)\n",
    "    print(f\"🎵 Top 5 Results:\")\n",
    "    for idx, (_, row) in enumerate(top_5.iterrows(), 1):\n",
    "        print(f\"   {idx}. {row['track']} by {row['artist']} (Score: {row['relevance_score']:.1f})\")\n",
    "else:\n",
    "    print(\"❌ No results found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "🔍 Query: 'period drama soundtrack'\n",
      "📊 Results: 54 tracks\n",
      "💬 LLM: These filters are set to find music that sounds like a 'period drama soundtrack', focusing on instrumental tracks with high acoustic quality and a more calm, less danceable feel. We've specifically included genres like soundtrack, orchestra, and classical to ensure relevant results.\n",
      "\n",
      "========================================\n",
      "🔍 Query: 'upbeat pop for workout'\n",
      "📊 Results: 518 tracks\n",
      "💬 LLM: I've set filters to find upbeat pop music ideal for workouts by prioritizing high danceability, energy, and positive mood, along with a fast tempo. Pop genre tracks will also receive a relevance boost.\n",
      "\n",
      "========================================\n",
      "🔍 Query: 'jazz for late night study'\n",
      "📊 Results: 2 tracks\n",
      "💬 LLM: These filters are set to find instrumental jazz music that is calm and mellow, perfect for a focused late-night study session. They prioritize lower energy, minimal vocals, and a quieter, more acoustic sound.\n",
      "\n",
      "========================================\n",
      "🔍 Query: 'ambient instrumental background music'\n",
      "📊 Results: 175 tracks\n",
      "💬 LLM: These filters are set to find ambient and instrumental music suitable for background listening. They prioritize tracks with minimal vocals and high instrumental content, low energy, slow tempo, and generally non-explicit language.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Try different queries\n",
    "test_queries = [\n",
    "    \"period drama soundtrack\",\n",
    "    \"upbeat pop for workout\",\n",
    "    \"jazz for late night study\",\n",
    "    \"ambient instrumental background music\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"🔍 Query: '{query}'\")\n",
    "    \n",
    "    try:\n",
    "        # Get LLM response\n",
    "        prompt = llm_service.create_initial_prompt(query)\n",
    "        filters = await llm_service.query_llm(prompt)\n",
    "        \n",
    "        # Get results\n",
    "        result = music_service.search(filters)\n",
    "        count = len(result[\"results\"])\n",
    "        \n",
    "        print(f\"📊 Results: {count} tracks\")\n",
    "        print(f\"💬 LLM: {filters.get('user_message', 'No message')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Service Component Debugging\n",
    "\n",
    "Test individual components of music_service and llm_service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Music Service Debugging\n",
    "print(\"🎵 MUSIC SERVICE DEBUGGING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Check dataset loading\n",
    "print(f\"📊 Dataset loaded: {music_service.main_df is not None}\")\n",
    "if music_service.main_df is not None:\n",
    "    print(f\"   Shape: {music_service.main_df.shape}\")\n",
    "    print(f\"   Columns: {list(music_service.main_df.columns[:10])}...\")  # First 10 columns\n",
    "\n",
    "# Test 2: Manual filter creation\n",
    "print(f\"\\n🔧 Testing manual filters...\")\n",
    "manual_filters = {\n",
    "    'danceability_min_decile': 5,\n",
    "    'danceability_max_decile': 10,\n",
    "    'danceability_decile_weight': 30,\n",
    "    'energy_min_decile': 7,\n",
    "    'energy_max_decile': 10,\n",
    "    'energy_decile_weight': 50,\n",
    "    'speechiness_min_decile': 1,\n",
    "    'speechiness_max_decile': 3,\n",
    "    'speechiness_decile_weight': -30,\n",
    "    'acousticness_min_decile': 1,\n",
    "    'acousticness_max_decile': 10,\n",
    "    'acousticness_decile_weight': 0,\n",
    "    'instrumentalness_min_decile': 7,\n",
    "    'instrumentalness_max_decile': 10,\n",
    "    'instrumentalness_decile_weight': 60,\n",
    "    'liveness_min_decile': 1,\n",
    "    'liveness_max_decile': 10,\n",
    "    'liveness_decile_weight': 0,\n",
    "    'valence_min_decile': 1,\n",
    "    'valence_max_decile': 10,\n",
    "    'valence_decile_weight': 0,\n",
    "    'views_min_decile': 1,\n",
    "    'views_max_decile': 10,\n",
    "    'views_decile_weight': 0,\n",
    "    'loudness_min': -60,\n",
    "    'loudness_max': 0,\n",
    "    'loudness_decile_weight': 0,\n",
    "    'tempo_min': 120,\n",
    "    'tempo_max': 140,\n",
    "    'tempo_decile_weight': 20,\n",
    "    'duration_ms_min': 30000,\n",
    "    'duration_ms_max': 600000,\n",
    "    'duration_ms_decile_weight': 0,\n",
    "    'album_release_year_min': 2010,\n",
    "    'album_release_year_max': 2025,\n",
    "    'track_is_explicit_min': 0,\n",
    "    'track_is_explicit_max': 1,\n",
    "    'spotify_artist_genres_include_any': 'electronic,edm',\n",
    "    'spotify_artist_genres_exclude_any': '',\n",
    "    'spotify_artist_genres_boosted': 'ambient,lo-fi',\n",
    "    'debug_tag': 'manual_test',\n",
    "    'reflection': 'Manual test filters',\n",
    "    'user_message': 'Testing manual filters'\n",
    "}\n",
    "\n",
    "# Test filter application\n",
    "filter_mask = music_service.llm_to_filters(manual_filters)\n",
    "print(f\"   Filter mask created: {type(filter_mask)}, {filter_mask.sum()} tracks match\")\n",
    "\n",
    "# Test results generation\n",
    "results_df = music_service.filters_to_results_df(filter_mask, manual_filters)\n",
    "print(f\"   Results generated: {len(results_df)} tracks with scores\")\n",
    "\n",
    "# Test summary creation\n",
    "summary = music_service.make_summary(results_df)\n",
    "print(f\"   Summary created: {list(summary.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Service Debugging\n",
    "print(\"🤖 LLM SERVICE DEBUGGING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test 1: Check client initialization\n",
    "print(f\"🔌 LLM client initialized: {llm_service.client is not None}\")\n",
    "\n",
    "# Test 2: Test prompt creation\n",
    "test_query = \"sad piano music\"\n",
    "initial_prompt = llm_service.create_initial_prompt(test_query)\n",
    "print(f\"\\n📝 Initial prompt created: {len(initial_prompt)} characters\")\n",
    "print(f\"   Preview: {initial_prompt[:100]}...\")\n",
    "\n",
    "# Test 3: Test refinement prompt\n",
    "dummy_filters = {'test': 'data'}\n",
    "dummy_summary = {'result_count': 100}\n",
    "refine_prompt = llm_service.create_refine_prompt(\n",
    "    original_query=test_query,\n",
    "    previous_filters=dummy_filters,\n",
    "    result_summary=dummy_summary,\n",
    "    user_feedback=\"make it more upbeat\"\n",
    ")\n",
    "print(f\"\\n🔄 Refinement prompt created: {len(refine_prompt)} characters\")\n",
    "print(f\"   Preview: {refine_prompt[:100]}...\")\n",
    "\n",
    "# Test 4: Test actual LLM call (if you want to test with real API)\n",
    "print(f\"\\n⚡ Testing LLM call...\")\n",
    "try:\n",
    "    llm_response = await llm_service.query_llm(initial_prompt)\n",
    "    print(f\"   ✅ LLM responded successfully\")\n",
    "    print(f\"   Response keys: {list(llm_response.keys())}\")\n",
    "    print(f\"   User message: {llm_response.get('user_message', 'None')}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ LLM call failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Storage Persistence Debugging\n",
    "\n",
    "Inspect and manipulate the in-memory storage state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Storage Debugging\nimport api.storage as storage\nfrom api.models import JobData, JobStatus, ConversationHistory, RefinementStep\nfrom datetime import datetime\nimport uuid\n\nprint(\"💾 STORAGE DEBUGGING\")\nprint(\"=\"*50)\n\n# Check initial state\nprint(f\"📊 Initial storage state:\")\nprint(f\"   JOB_STORE entries: {len(storage.JOB_STORE)}\")\nprint(f\"   RESULT_STORE entries: {len(storage.RESULT_STORE)}\")\n\n# Create test job data\ntest_job_id = str(uuid.uuid4())\ntest_job_data = JobData(\n    status=JobStatus.QUEUED,\n    query_text=\"test query for storage\",\n    started_at=datetime.now(),\n    finished_at=None,\n    error_message=None,\n    conversation_history=None,\n    current_filters_json=None,\n    result_count=None\n)\n\nprint(f\"\\n🧪 Testing storage operations...\")\n\n# Test store job\nstorage.store_job(test_job_id, test_job_data)\nprint(f\"   ✅ Job stored with ID: {test_job_id[:8]}...\")\n\n# Test retrieve job\nretrieved_job = storage.get_job(test_job_id)\nprint(f\"   ✅ Job retrieved: {retrieved_job.query_text}\")\n\n# Test job exists\nexists = storage.job_exists(test_job_id)\nprint(f\"   ✅ Job exists check: {exists}\")\n\n# Check storage state after operations\nprint(f\"\\n📊 Storage state after test:\")\nprint(f\"   JOB_STORE entries: {len(storage.JOB_STORE)}\")\nprint(f\"   Job IDs: {list(storage.JOB_STORE.keys())}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversation history storage\n",
    "print(\"💬 Testing conversation history storage...\")\n",
    "\n",
    "# Create test conversation history\n",
    "test_step = RefinementStep(\n",
    "    step_number=1,\n",
    "    step_type=\"initial\",\n",
    "    user_input=\"test query\",\n",
    "    filters_json={\"test\": \"filters\"},\n",
    "    result_count=42,\n",
    "    user_message=\"Test user message\",\n",
    "    rationale=\"Test rationale\",\n",
    "    result_summary={\"result_count\": 42},\n",
    "    timestamp=datetime.now(),\n",
    "    target_range=\"50-150\"\n",
    ")\n",
    "\n",
    "test_conversation = ConversationHistory(\n",
    "    original_query=\"test query\",\n",
    "    steps=[test_step],\n",
    "    current_step=1,\n",
    "    total_auto_refinements=0\n",
    ")\n",
    "\n",
    "# Update job with conversation history\n",
    "test_job_data.conversation_history = test_conversation\n",
    "storage.store_job(test_job_id, test_job_data)\n",
    "\n",
    "# Retrieve and verify\n",
    "updated_job = storage.get_job(test_job_id)\n",
    "print(f\"   ✅ Conversation history stored\")\n",
    "print(f\"   Steps: {len(updated_job.conversation_history.steps)}\")\n",
    "print(f\"   Original query: {updated_job.conversation_history.original_query}\")\n",
    "print(f\"   Step 1 message: {updated_job.conversation_history.steps[0].user_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 CURRENT STORAGE STATE\n",
      "========================================\n",
      "📋 Jobs in storage: 1\n",
      "   🆔 7f703445... - queued - 'test query for storage'\n",
      "📊 Results in storage: 0\n"
     ]
    }
   ],
   "source": [
    "# Storage inspection utility\n",
    "def inspect_storage():\n",
    "    \"\"\"Utility function to inspect current storage state\"\"\"\n",
    "    print(\"🔍 CURRENT STORAGE STATE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"📋 Jobs in storage: {len(storage.JOB_STORE)}\")\n",
    "    for job_id, job_data in storage.JOB_STORE.items():\n",
    "        print(f\"   🆔 {job_id[:8]}... - {job_data.status.value} - '{job_data.query_text}'\")\n",
    "        if job_data.conversation_history:\n",
    "            steps = len(job_data.conversation_history.steps)\n",
    "            print(f\"      💬 {steps} conversation steps\")\n",
    "    \n",
    "    print(f\"📊 Results in storage: {len(storage.RESULT_STORE)}\")\n",
    "    for job_id, results in storage.RESULT_STORE.items():\n",
    "        print(f\"   🆔 {job_id[:8]}... - {results.result_count} total, {len(results.tracks)} tracks returned\")\n",
    "\n",
    "# Run inspection\n",
    "inspect_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DEBUG: Let's add error logging to find the issue\nimport traceback\n\nprint(\"🔧 DEBUGGING SEARCH SERVICE\")\nprint(\"=\"*50)\n\n# Test service initialization\nprint(\"Testing service initialization...\")\ntry:\n    from api.search_service import initialize_services, music_service, llm_service\n    print(f\"   Music service main_df loaded: {music_service.main_df is not None}\")\n    print(f\"   LLM service client loaded: {llm_service.client is not None}\")\n\n    # Test if services need initialization\n    if music_service.main_df is None:\n        print(\"   ⚠️  Music service not initialized!\")\n        music_service.initialize('../data/main_df.csv')\n        print(f\"   ✅ Music service initialized: {music_service.main_df is not None}\")\n\n    if llm_service.client is None:\n        print(\"   ⚠️  LLM service not initialized!\")\n        llm_service.initialize()\n        print(f\"   ✅ LLM service initialized: {llm_service.client is not None}\")\n\nexcept Exception as e:\n    print(f\"   ❌ Service initialization error: {str(e)}\")\n    traceback.print_exc()\n\n# Test a simple LLM call\nprint(f\"\\n🤖 Testing LLM call...\")\ntry:\n    test_prompt = llm_service.create_initial_prompt(\"test music\")\n    test_response = await llm_service.query_llm(test_prompt)\n    print(f\"   ✅ LLM call successful: {list(test_response.keys())}\")\nexcept Exception as e:\n    print(f\"   ❌ LLM call failed: {str(e)}\")\n    traceback.print_exc()\n\n# Test music search\nprint(f\"\\n🎵 Testing music search...\")\ntry:\n    if 'test_response' in locals():\n        search_result = music_service.search(test_response)\n        print(f\"   ✅ Music search successful: {len(search_result['results'])} results\")\n    else:\n        print(\"   ⏭️  Skipping music search (no LLM response)\")\nexcept Exception as e:\n    print(f\"   ❌ Music search failed: {str(e)}\")\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: End-to-End API Testing\n",
    "\n",
    "Simulate full API endpoint behavior with storage inspection at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# End-to-End API Simulation\nfrom api.search_service import create_search_job, get_job_status, process_search_job\nfrom api.models import SearchRequest\nfrom fastapi import BackgroundTasks\nimport asyncio\n\nprint(\"🚀 END-TO-END API TESTING\")\nprint(\"=\"*50)\n\n# Initialize services for search_service\nfrom api.search_service import initialize_services\n# Note: We've already initialized individual services above\n#initialize_services()\n\n\n# Clear storage for clean test\nstorage.JOB_STORE.clear()\nstorage.RESULT_STORE.clear()\nprint(\"🧹 Cleared storage for clean test\")\n\n# Test 1: Simulate POST /search endpoint\nprint(\"📤 Testing POST /search endpoint simulation...\")\n\ntest_request = SearchRequest(query_text=\"period drama soundtrack\")\n\n# Mock BackgroundTasks (since we're not in FastAPI context)\nclass MockBackgroundTasks:\n    def __init__(self):\n        self.tasks = []\n    \n    def add_task(self, func, *args, **kwargs):\n        self.tasks.append((func, args, kwargs))\n        print(f\"   📋 Background task added: {func.__name__}\")\n\nmock_bg_tasks = MockBackgroundTasks()\n\n# Call create_search_job\nresponse = await create_search_job(test_request, mock_bg_tasks)\njob_id = response[\"job_id\"]\n\nprint(f\"   ✅ Job created with ID: {job_id[:8]}...\")\nprint(f\"   📋 Background tasks queued: {len(mock_bg_tasks.tasks)}\")\n\n# Inspect storage after job creation\nprint(\"🔍 Storage after job creation:\")\ninspect_storage()"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Testing GET /jobs/{id} endpoint (before processing)...\n",
      "   ✅ Job status retrieved\n",
      "   Status: JobStatus.QUEUED\n",
      "   Query: period drama soundtrack\n",
      "   Results: None\n",
      "   Conversation history: None\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Simulate GET /jobs/{id} endpoint (before processing)\n",
    "print(\"📥 Testing GET /jobs/{id} endpoint (before processing)...\")\n",
    "\n",
    "job_response = await get_job_status(job_id)\n",
    "print(f\"   ✅ Job status retrieved\")\n",
    "print(f\"   Status: {job_response.status}\")\n",
    "print(f\"   Query: {job_response.query_text}\")\n",
    "print(f\"   Results: {job_response.results}\")\n",
    "print(f\"   Conversation history: {job_response.conversation_history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Simulating background job processing...\n",
      "   🏃 Executing: process_search_job with args ('c4caf0ab-7400-4b0f-9d18-a519df7c9359', 'period drama soundtrack')\n",
      "   ✅ Background processing completed\n",
      "🔍 Storage after background processing:\n",
      "🔍 CURRENT STORAGE STATE\n",
      "========================================\n",
      "📋 Jobs in storage: 1\n",
      "   🆔 c4caf0ab... - done - 'period drama soundtrack'\n",
      "      💬 3 conversation steps\n",
      "📊 Results in storage: 1\n",
      "   🆔 c4caf0ab... - 197 total, 50 tracks returned\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Simulate background job processing\n",
    "print(\"⚙️ Simulating background job processing...\")\n",
    "\n",
    "# Manually execute the background task\n",
    "if mock_bg_tasks.tasks:\n",
    "    func, args, kwargs = mock_bg_tasks.tasks[0]\n",
    "    print(f\"   🏃 Executing: {func.__name__} with args {args}\")\n",
    "    \n",
    "    # Execute the background processing\n",
    "    await func(*args, **kwargs)\n",
    "    \n",
    "    print(f\"   ✅ Background processing completed\")\n",
    "\n",
    "# Inspect storage after processing\n",
    "print(\"🔍 Storage after background processing:\")\n",
    "inspect_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Testing GET /jobs/{id} endpoint (after processing)...\n",
      "   ✅ Final job status retrieved\n",
      "   Status: JobStatus.DONE\n",
      "   Result count: 197\n",
      "   Has results: True\n",
      "   Has conversation history: True\n",
      "   📜 Conversation steps: 3\n",
      "   🔄 Auto refinements: 2\n",
      "      Step 1 (initial): 202 results\n",
      "         💬 These filters aim to find music that sounds like a 'period drama soundtrack' by prioritizing instrumental, acoustic, and classical-sounding tracks, while excluding modern, dance-oriented, or vocal-heavy genres.\n",
      "      Step 2 (auto_refine): 202 results\n",
      "         💬 These filters are refined to strongly prioritize instrumental, acoustic, and classical-style music typical of period drama soundtracks, by more strictly excluding upbeat, vocal, or overly cheerful tracks. We've also added a preference for slower tempos and boosted more popular tracks to enhance relevance and quality.\n",
      "      Step 3 (auto_refine): 197 results\n",
      "         💬 These refined filters more precisely target period drama soundtracks by strictly favoring slower tempos, lower energy, and more neutral to melancholic moods. We've also maintained a strong preference for acoustic, instrumental, and classical-style music to ensure high relevance.\n",
      "   🎵 Top 3 tracks:\n",
      "      1. Nocturne No. 2 In E Flat, Op. 9 No. 2 by Frédéric Chopin (Score: 6409387861.4)\n",
      "      2. Clair de Lune, L. 32 by Claude Debussy (Score: 2644725762.4)\n",
      "      3. Danse Macabre, Op. 40, R.171 by Camille Saint-Saëns (Score: 2124099726.3)\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Simulate GET /jobs/{id} endpoint (after processing)\n",
    "print(\"📥 Testing GET /jobs/{id} endpoint (after processing)...\")\n",
    "\n",
    "final_job_response = await get_job_status(job_id)\n",
    "print(f\"   ✅ Final job status retrieved\")\n",
    "print(f\"   Status: {final_job_response.status}\")\n",
    "print(f\"   Result count: {final_job_response.result_count}\")\n",
    "print(f\"   Has results: {final_job_response.results is not None}\")\n",
    "print(f\"   Has conversation history: {final_job_response.conversation_history is not None}\")\n",
    "\n",
    "if final_job_response.conversation_history:\n",
    "    history = final_job_response.conversation_history\n",
    "    print(f\"   📜 Conversation steps: {len(history.steps)}\")\n",
    "    print(f\"   🔄 Auto refinements: {history.total_auto_refinements}\")\n",
    "    \n",
    "    for i, step in enumerate(history.steps):\n",
    "        print(f\"      Step {i+1} ({step.step_type}): {step.result_count} results\")\n",
    "        print(f\"         💬 {step.user_message}\")\n",
    "\n",
    "if final_job_response.results:\n",
    "    results = final_job_response.results\n",
    "    print(f\"   🎵 Top 3 tracks:\")\n",
    "    for i, track in enumerate(results.tracks[:3]):\n",
    "        print(f\"      {i+1}. {track.track} by {track.artist} (Score: {track.relevance_score:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Testing error handling...\n",
      "   ✅ Correctly handled non-existent job: HTTPException\n",
      "🏁 End-to-end testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Error handling simulation\n",
    "print(\"❌ Testing error handling...\")\n",
    "\n",
    "# Test non-existent job\n",
    "fake_job_id = \"non-existent-job-id\"\n",
    "try:\n",
    "    await get_job_status(fake_job_id)\n",
    "    print(f\"   ❌ Should have failed for non-existent job\")\n",
    "except Exception as e:\n",
    "    print(f\"   ✅ Correctly handled non-existent job: {type(e).__name__}\")\n",
    "\n",
    "print(\"🏁 End-to-end testing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 FINAL STORAGE STATE\n",
      "==================================================\n",
      "🔍 CURRENT STORAGE STATE\n",
      "========================================\n",
      "📋 Jobs in storage: 1\n",
      "   🆔 c4caf0ab... - done - 'period drama soundtrack'\n",
      "      💬 3 conversation steps\n",
      "📊 Results in storage: 1\n",
      "   🆔 c4caf0ab... - 197 total, 50 tracks returned\n",
      "⚡ PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "   Job c4caf0ab... took 81.23 seconds\n",
      "      3 total steps, 2 auto-refinements\n"
     ]
    }
   ],
   "source": [
    "# Final storage inspection\n",
    "print(\"📊 FINAL STORAGE STATE\")\n",
    "print(\"=\"*50)\n",
    "inspect_storage()\n",
    "\n",
    "# Performance summary\n",
    "print(\"⚡ PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for job_id, job_data in storage.JOB_STORE.items():\n",
    "    if job_data.started_at and job_data.finished_at:\n",
    "        duration = job_data.finished_at - job_data.started_at\n",
    "        print(f\"   Job {job_id[:8]}... took {duration.total_seconds():.2f} seconds\")\n",
    "        if job_data.conversation_history:\n",
    "            steps = len(job_data.conversation_history.steps)\n",
    "            auto_refines = job_data.conversation_history.total_auto_refinements\n",
    "            print(f\"      {steps} total steps, {auto_refines} auto-refinements\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Utility functions for image upload debugging\ndef test_image_service():\n    \"\"\"Test the image validation service\"\"\"\n    print(\"🔧 TESTING IMAGE SERVICE\")\n    print(\"=\"*30)\n    \n    from image_service import image_service\n    \n    if image_path.exists():\n        print(f\"📁 Testing with: {image_path.name}\")\n        \n        # Test image validation pipeline\n        try:\n            # Read file as UploadFile would\n            with open(image_path, 'rb') as f:\n                file_content = f.read()\n            \n            print(f\"   📏 File size: {len(file_content)} bytes\")\n            print(f\"   🔍 Validating image...\")\n            \n            # Mock UploadFile object\n            class MockUploadFile:\n                def __init__(self, content, filename, content_type):\n                    self.content = content\n                    self.filename = filename\n                    self.content_type = content_type\n                    self.size = len(content)\n                \n                async def read(self):\n                    return self.content\n            \n            mock_file = MockUploadFile(\n                content=file_content,\n                filename=\"bridg-preview.png\",\n                content_type=\"image/png\"\n            )\n            \n            # Test validation\n            base64_data, temp_file_id = await image_service.validate_and_process_image(mock_file)\n            print(f\"   ✅ Validation successful!\")\n            print(f\"   📊 Base64 length: {len(base64_data)}\")\n            print(f\"   🆔 Temp file ID: {temp_file_id}\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"   ❌ Validation failed: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return False\n    else:\n        print(f\"   ❌ Test image not found\")\n        return False\n\ndef debug_google_genai_formats():\n    \"\"\"Debug different ways to format content for Google GenAI\"\"\"\n    print(\"🔍 DEBUGGING GOOGLE GENAI FORMATS\")\n    print(\"=\"*40)\n    \n    if not base64_image_data:\n        print(\"   ⏭️ No image data to test\")\n        return\n    \n    from google.genai.types import Content, Part\n    import google.genai as genai\n    \n    test_text = \"Analyze this image\"\n    \n    # Test different content formats\n    formats_to_test = [\n        \"String format\",\n        \"Content with from_text and from_bytes\",\n        \"Content with dictionary parts\",\n        \"Mixed content list\"\n    ]\n    \n    for i, format_name in enumerate(formats_to_test, 1):\n        print(f\"\\\\n   {i}. Testing {format_name}...\")\n        \n        try:\n            if format_name == \"String format\":\n                content = test_text\n                \n            elif format_name == \"Content with from_text and from_bytes\":\n                content = Content(\n                    role=\"user\",\n                    parts=[\n                        Part.from_text(test_text),\n                        Part.from_bytes(\n                            data=base64.b64decode(base64_image_data),\n                            mime_type=\"image/jpeg\"\n                        )\n                    ]\n                )\n                \n            elif format_name == \"Content with dictionary parts\":\n                content = Content(\n                    role=\"user\",\n                    parts=[\n                        {\"text\": test_text},\n                        {\"inline_data\": {\"mime_type\": \"image/jpeg\", \"data\": base64_image_data}}\n                    ]\n                )\n                \n            elif format_name == \"Mixed content list\":\n                content = [\n                    test_text,\n                    {\"mime_type\": \"image/jpeg\", \"data\": base64_image_data}\n                ]\n            \n            print(f\"      ✅ Format created successfully: {type(content)}\")\n            \n        except Exception as e:\n            print(f\"      ❌ Format failed: {str(e)}\")\n\n# Run utility tests\nif image_path.exists():\n    print(\"🧪 RUNNING IMAGE SERVICE TESTS\")\n    print(\"=\"*50)\n    test_image_service()\n    debug_google_genai_formats()\nelse:\n    print(\"💡 Please add bridg-preview.png to frontend/src/assets/ to run image tests\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test full end-to-end image upload simulation\nif base64_image_data:\n    print(\"🚀 END-TO-END IMAGE UPLOAD SIMULATION\")\n    print(\"=\"*50)\n    \n    # Clear storage for clean test\n    storage.JOB_STORE.clear()\n    storage.RESULT_STORE.clear()\n    print(\"🧹 Cleared storage for clean image test\")\n    \n    # Create search request with image data\n    from models import SearchRequest\n    \n    multimodal_request = SearchRequest(\n        query_text=\"Music for a period drama set in Elizabethan England\",\n        image_data=base64_image_data\n    )\n    \n    print(f\"📤 Creating multimodal search request...\")\n    print(f\"   Query: '{multimodal_request.query_text}'\")\n    print(f\"   Image data: {len(multimodal_request.image_data)} characters\")\n    \n    # Mock background tasks\n    class MockBackgroundTasks:\n        def __init__(self):\n            self.tasks = []\n        \n        def add_task(self, func, *args, **kwargs):\n            self.tasks.append((func, args, kwargs))\n            print(f\"   📋 Background task added: {func.__name__}\")\n    \n    mock_bg_tasks = MockBackgroundTasks()\n    \n    # Test job creation\n    try:\n        response = await create_search_job(multimodal_request, mock_bg_tasks)\n        job_id = response[\"job_id\"]\n        print(f\"   ✅ Multimodal job created: {job_id[:8]}...\")\n        \n        # Execute background task\n        if mock_bg_tasks.tasks:\n            func, args, kwargs = mock_bg_tasks.tasks[0]\n            print(f\"   🏃 Executing background task...\")\n            \n            # This should test our image processing pipeline\n            await func(*args, **kwargs)\n            print(f\"   ✅ Background processing completed\")\n            \n            # Check final results\n            final_response = await get_job_status(job_id)\n            print(f\"\\\\n📊 FINAL MULTIMODAL RESULTS:\")\n            print(f\"   Status: {final_response.status}\")\n            print(f\"   Result count: {final_response.result_count}\")\n            \n            if final_response.results and len(final_response.results.tracks) > 0:\n                print(f\"   🎵 Top 3 multimodal results:\")\n                for i, track in enumerate(final_response.results.tracks[:3]):\n                    print(f\"      {i+1}. {track.track} by {track.artist} (Score: {track.relevance_score:.1f})\")\n                    \n                # Show LLM's understanding of the image\n                if final_response.conversation_history and final_response.conversation_history.steps:\n                    first_step = final_response.conversation_history.steps[0]\n                    print(f\"\\\\n🧠 LLM's multimodal analysis:\")\n                    print(f\"   💬 {first_step.user_message}\")\n                    if len(first_step.rationale) > 0:\n                        print(f\"   🤔 Reasoning: {first_step.rationale[:300]}...\")\n                \n            else:\n                print(\"   ❌ No results generated\")\n            \n    except Exception as e:\n        print(f\"   ❌ Multimodal job processing failed: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nelse:\n    print(\"⏭️ Skipping end-to-end multimodal test (no image loaded)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Test full end-to-end image upload simulation\nif base64_image_data:\n    print(\"🚀 END-TO-END IMAGE UPLOAD SIMULATION\")\n    print(\"=\"*50)\n    \n    # Clear storage for clean test\n    storage.JOB_STORE.clear()\n    storage.RESULT_STORE.clear()\n    print(\"🧹 Cleared storage for clean image test\")\n    \n    # Create search request with image data\n    from api.models import SearchRequest\n    from api.search_service import create_search_job, get_job_status\n    \n    multimodal_request = SearchRequest(\n        query_text=\"Music for a period drama set in Elizabethan England\",\n        image_data=base64_image_data\n    )\n    \n    print(f\"📤 Creating multimodal search request...\")\n    print(f\"   Query: '{multimodal_request.query_text}'\")\n    print(f\"   Image data: {len(multimodal_request.image_data)} characters\")\n    \n    # Mock background tasks\n    class MockBackgroundTasks:\n        def __init__(self):\n            self.tasks = []\n        \n        def add_task(self, func, *args, **kwargs):\n            self.tasks.append((func, args, kwargs))\n            print(f\"   📋 Background task added: {func.__name__}\")\n    \n    mock_bg_tasks = MockBackgroundTasks()\n    \n    # Test job creation\n    try:\n        response = await create_search_job(multimodal_request, mock_bg_tasks)\n        job_id = response[\"job_id\"]\n        print(f\"   ✅ Multimodal job created: {job_id[:8]}...\")\n        \n        # Execute background task\n        if mock_bg_tasks.tasks:\n            func, args, kwargs = mock_bg_tasks.tasks[0]\n            print(f\"   🏃 Executing background task...\")\n            \n            # This should test our image processing pipeline\n            await func(*args, **kwargs)\n            print(f\"   ✅ Background processing completed\")\n            \n            # Check final results\n            final_response = await get_job_status(job_id)\n            print(f\"\\n📊 FINAL MULTIMODAL RESULTS:\")\n            print(f\"   Status: {final_response.status}\")\n            print(f\"   Result count: {final_response.result_count}\")\n            \n            if final_response.results and len(final_response.results.tracks) > 0:\n                print(f\"   🎵 Top 3 multimodal results:\")\n                for i, track in enumerate(final_response.results.tracks[:3]):\n                    print(f\"      {i+1}. {track.track} by {track.artist} (Score: {track.relevance_score:.1f})\")\n                    \n                # Show LLM's understanding of the image\n                if final_response.conversation_history and final_response.conversation_history.steps:\n                    first_step = final_response.conversation_history.steps[0]\n                    print(f\"\\n🧠 LLM's multimodal analysis:\")\n                    print(f\"   💬 {first_step.user_message}\")\n                    if len(first_step.rationale) > 0:\n                        print(f\"   🤔 Reasoning: {first_step.rationale[:300]}...\")\n                \n            else:\n                print(\"   ❌ No results generated\")\n            \n    except Exception as e:\n        print(f\"   ❌ Multimodal job processing failed: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nelse:\n    print(\"⏭️ Skipping end-to-end multimodal test (no image loaded)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Utility functions for image upload debugging\nimport asyncio\n\nasync def test_image_service():\n    \"\"\"Test the image validation service\"\"\"\n    print(\"🔧 TESTING IMAGE SERVICE\")\n    print(\"=\"*30)\n    \n    from api.image_service import image_service\n    \n    if image_path.exists():\n        print(f\"📁 Testing with: {image_path.name}\")\n        \n        # Test image validation pipeline\n        try:\n            # Read file as UploadFile would\n            with open(image_path, 'rb') as f:\n                file_content = f.read()\n            \n            print(f\"   📏 File size: {len(file_content)} bytes\")\n            print(f\"   🔍 Validating image...\")\n            \n            # Mock UploadFile object\n            class MockUploadFile:\n                def __init__(self, content, filename, content_type):\n                    self.content = content\n                    self.filename = filename\n                    self.content_type = content_type\n                    self.size = len(content)\n                \n                async def read(self):\n                    return self.content\n            \n            mock_file = MockUploadFile(\n                content=file_content,\n                filename=\"bridg-preview.png\",\n                content_type=\"image/png\"\n            )\n            \n            # Test validation\n            base64_data, temp_file_id = await image_service.validate_and_process_image(mock_file)\n            print(f\"   ✅ Validation successful!\")\n            print(f\"   📊 Base64 length: {len(base64_data)}\")\n            print(f\"   🆔 Temp file ID: {temp_file_id}\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"   ❌ Validation failed: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            return False\n    else:\n        print(f\"   ❌ Test image not found\")\n        return False\n\ndef debug_google_genai_formats():\n    \"\"\"Debug different ways to format content for Google GenAI\"\"\"\n    print(\"🔍 DEBUGGING GOOGLE GENAI FORMATS\")\n    print(\"=\"*40)\n    \n    if not base64_image_data:\n        print(\"   ⏭️ No image data to test\")\n        return\n    \n    from google.genai.types import Content, Part\n    import google.genai as genai\n    \n    test_text = \"Analyze this image\"\n    \n    # Test different content formats\n    formats_to_test = [\n        \"String format\",\n        \"Content with from_text and from_bytes\",\n        \"Content with dictionary parts\",\n        \"Mixed content list\"\n    ]\n    \n    for i, format_name in enumerate(formats_to_test, 1):\n        print(f\"\\n   {i}. Testing {format_name}...\")\n        \n        try:\n            if format_name == \"String format\":\n                content = test_text\n                \n            elif format_name == \"Content with from_text and from_bytes\":\n                content = Content(\n                    role=\"user\",\n                    parts=[\n                        Part.from_text(test_text),\n                        Part.from_bytes(\n                            data=base64.b64decode(base64_image_data),\n                            mime_type=\"image/jpeg\"\n                        )\n                    ]\n                )\n                \n            elif format_name == \"Content with dictionary parts\":\n                content = Content(\n                    role=\"user\",\n                    parts=[\n                        {\"text\": test_text},\n                        {\"inline_data\": {\"mime_type\": \"image/jpeg\", \"data\": base64_image_data}}\n                    ]\n                )\n                \n            elif format_name == \"Mixed content list\":\n                content = [\n                    test_text,\n                    {\"mime_type\": \"image/jpeg\", \"data\": base64_image_data}\n                ]\n            \n            print(f\"      ✅ Format created successfully: {type(content)}\")\n            \n        except Exception as e:\n            print(f\"      ❌ Format failed: {str(e)}\")\n\n# Run utility tests - using asyncio.run() for compatibility\nasync def run_image_tests():\n    if image_path.exists():\n        print(\"🧪 RUNNING IMAGE SERVICE TESTS\")\n        print(\"=\"*50)\n        await test_image_service()\n        debug_google_genai_formats()\n    else:\n        print(\"💡 Please add bridg-preview.png to frontend/src/assets/ to run image tests\")\n\n# Execute the tests using asyncio.run() for compatibility\ntry:\n    asyncio.run(run_image_tests())\nexcept RuntimeError as e:\n    if \"asyncio.run() cannot be called from a running event loop\" in str(e):\n        # If we're already in an async context, use await\n        await run_image_tests()\n    else:\n        print(f\"Error running tests: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Section 5: Image Upload Testing & Debugging\n\nTest the complete image upload flow including multimodal LLM integration.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}